#!/usr/bin/env python3
__author__ = 'Felipe Cordeiro'
__title_ = 'nasa_dataset_analysis.py'
__usage__ = 'bin/spark-submit --master local <path>/nasa_dataset_analysis.py'

'''
This script count the number of hits and bytes generate by the public NASA web server in 1995.
'''

from pyspark import SparkConf, SparkContext

logFile = "/Users/Felipe/PycharmProjects/spark_project/access_log_Jul95.log"
sc = SparkContext("local", "Nasa Dataset Analysis")
logData = sc.textFile(logFile).cache()

hits = logData.count()
server_messages = [100, 101, 200, 201, 202, 203, 204, 205, 206, 300, 301, 302, 303, 304, 305, 306, 307, 400, 401, 402,
                   403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 500, 501, 502, 503, 504, 505]

def validation(value):
    if value in server_messages:
        print("It is not a size, but a error code message:", value)
        return 0

def processing(x):

    try:
        if len(x) == 10 and validation(x[9]) != 0:
            return int(x[9])
        if len(x) == 12 and validation(x[11]) != 0:
            return int(x[11])
        if len(x) == 8 and validation(x[7]) != 0:
            return int(x[7])

    except IndexError:
            print("Index error:",x)
    except ValueError:
        # If some line is out of the handled pattern.
        if len(x) > 12 or len(x) == 11 or len(x) == 9 or len(x) < 8:
            print(x)
        return 0;

def sum(a, b):
    if a is None:
        a = 0
    if b is None:
        b = 0
    return a + b

size = logData.map(lambda line: line.split(" ")).map(lambda line: processing(line))
total_size = size.reduce(lambda a, b: sum(a, b))

print("Total number of bytes generated by the web-server: ",total_size)
print "Number of hits: %i" % hits